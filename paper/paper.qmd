---
title: "Election 2024"
author: "Alexander Guarasci & Jacob Gilbert"
date: today
date-format: long
bibliography: references.bib
execute:
  echo: false
format: pdf
fig-pos: "H"
abstract: |

 This paper is about the election
---

```{python}
#| echo: false
import pandas as pd
import matplotlib.pyplot as plt
import jupyter
from tabulate import tabulate
```

# 1 Introduction

This paper develops a predictive model for the 2024 US Presidential Election using state-level polling data to forecast the likely winner between Donald Trump and Kamala Harris. By aggregating high-quality polls that account for recency and sample size, we create a logistic regression model that estimates the probability of a Trump or Harris victory in each state. This approach allows us to analyze voter support patterns across the states and predict election outcomes more accurately.

The estimand in this study is the probability that Donald Trump or Kamala Harris wins a given state, derived from aggregated state-level polling averages. The binary outcome variable in our model indicates whether Trump (1) or Harris (0) is predicted to win in each state, with predictor variables comprising the weighted average polling percentages for both candidates. We assign greater importance to more recent polls and those with larger sample sizes, enhancing the reliability of our estimates.

Our model employs a logistic regression framework to predict election outcomes based on these weighted averages. The results reveal the geographic distribution of support for each candidate. We have focused our analysis on the swing states where polling percentages are closely contested and could significantly influence the final election result.

Accurate election predictions provide valuable insights into voter dynamics, helping political analysts, campaigns, and the public anticipate electoral outcomes. By focusing on high-quality polling data and incorporating weights for recency and sample size, our model enhances prediction reliability and identifies key regions where voter sentiment may shift, ultimately impacting the election..

The structure of the paper is as follows

# Data {#sec-data}

The data used in this paper was gathered from FiveThirtyEight [@FiveThirtyEight] a website that aims to use "data and evidence to advance public knowledge". The programming language for data analysis, visualization and statistical investigation was Python [@Python] along with the packages...

# 2.1 Measurement


The voter support data used in this analysis comes from raw polling information sourced from the Project 538 online database [@FiveThirtyEight]. This dataset provides predictions for candidate support for the 2024 U.S. Presidential Election by aggregating various polls that capture public sentiment towards candidates Donald Trump and Kamala Harris. Each entry in the dataset corresponds to a specific question around voting preference in a poll conducted by different polling organisations, which measure the percentage of respondents expressing their support for each candidate in their respective state.
 
The polling data is collected through carefully structured survey questions, designed to elicit clear responses regarding voter preferences. By aggregating these individual responses, we are able to derive state-level sentiment for both candidates. This transformation of general voter sentiment into specific data points enables us to analyse the competitive landscape between Trump and Harris across swing states, providing a clearer understanding of electoral dynamics as they relate to the upcoming election.


# 2.1 The Dataset


A swing state in the context of a US election is defined as "a state where the number of Democratic and Republican voters is about the same, that has an important influence on the result of the election of the United States President". States that do not fall under his definition are not likely to "swing" to another party as we approach the election, and thus their outcome can be reasonably assumed. Thus the election result is normally decided by which party wins just a handful of states in the electoral college system. By concentrating on these swing states, we aim to capture the most critical and uncertain areas of the electoral map, where voter preferences are most likely to sway the final result. Thus our analysis restricts the datasets to state level polls in the following states: Arizona, Pennsylvania, North Carolina, Georgia, Nevada, Michigan, and Wisconsin [@nyt_swing, @538]. In figure 3 we see the ammount that each party hold according to those sources, with the remaining swing states, seen in the middle of this graph set to determine if either Harris or Trump recieve over 270th electoral votes.

```{python}
import matplotlib.pyplot as plt
import numpy as np

# Step 1: Define the Electoral Votes for Each State
states = ["California", "Connecticut", "Delaware", "Hawaii", "Illinois",
"Maine", "Maryland", "Massachusetts", "Minnesota", "New Jersey",
"New York", "Oregon", "Rhode Island", "Vermon", "Washington",
"District of Columbia", "New Mexico", "New Hampshire", "Colorado", "Virginia",
"Arizona",
"Georgia",
"Michigan",
"Nevada",
"North Carolina",
"Pennsylvania",
"Wisconsin",
"Alabama", "Alaska", "Arkansas", "Florida", "Idaho",
"Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana",
"Mississippi", "Missouri", "Montana", "Nebraska", "North Dakota",
"Ohio", "Oklahoma", "South Carolina", "South Dakota", "Tennessee",
"Texas", "Utah", "West Virginia", "Wyoming"]
print(len(states))
votes = [54, 7 , 2, 4, 19, 4, 10, 11, 10, 14, 28, 8, 4, 3, 12, 3, 5, 4, 10, 13,
         11,16,15,6,16,19,10,
         9,3,6,30,4,11,6,6,8,8,6,10,4,5,3,17,7,9,3,11,40,6,4,3]

print(len(votes))
# Step 2: Compute the Cumulative Electoral Votes
cumulative_votes = np.cumsum(votes)

# Compute Cumulative Votes in Reverse Order for Republicans
cumulative_votes_reversed = np.cumsum(votes[::-1])

# X-axis values (state index)
x = np.arange(len(cumulative_votes))

# Step 3: Plot the Step Function for Democrats (left to right)
plt.figure(figsize=(18, 8))
plt.step(x, cumulative_votes, where='post', color='navy', linewidth=2, marker='o')

# Reverse the x-axis for the Republican step function (right to left)
x_reversed = np.flip(x)
plt.step(x_reversed, cumulative_votes_reversed, where='post', color='red', linewidth=2, marker='o')

# Step 4: Customize the Plot
plt.xticks(range(len(states)), states, rotation=90, fontsize=8)
plt.yticks(np.arange(0, 550, 50))

# Fill the area under the curve with blue for Democrats (left to right)
threshold_index = np.argmax(cumulative_votes >= 227)  # First point where cumulative votes reach 270
plt.fill_between(x[:threshold_index + 1], cumulative_votes[:threshold_index + 1], 
                 step='post', color='blue', alpha=0.4)

# Fill the area under the curve with red for Republicans (right to left)
threshold_index = np.argmax(cumulative_votes >= 219)
plt.fill_between(x_reversed[:threshold_index + 5], cumulative_votes_reversed[:threshold_index + 5], step='post', color='red', alpha=0.4)

# Add a horizontal line at 270 votes (threshold to win)
plt.axhline(270, color='purple', linestyle='--', linewidth=2)
plt.text(len(states) / 2, 275, '270 votes to win', horizontalalignment='center', color='purple', fontsize=12)

# Add lines for current Democratic and Republican thresholds
plt.axhline(225, color='blue', linestyle='--', linewidth=1.5)
plt.text(210, 220, '225 votes', horizontalalignment='center', color='blue', fontsize=12)

# Add Labels and Title
plt.xlabel('Electoral College Markets (electoral votes)', fontsize=12)
plt.ylabel('Total Electoral Votes', fontsize=12)
plt.title('Step Function of Total Electoral Votes\nwith States Ordered by Vote Share', fontsize=15)

# Add Grid for Better Visibility
plt.grid(True, linestyle='--', alpha=0.5)

# Adjust Layout for Better Fit
plt.tight_layout()

# Step 5: Show the Plot
plt.show()


```


It’s important to note that each pollster employs unique sampling techniques and methodologies that influence their Pollscore and Transparency Score. Pollscore is a measure of a pollster's historical accuracy, reflecting the quality of their sampling methods and data collection. a good score reflecting low propensity to be impacted by errors and biases that can occur in survey sampling. It also accounts for impacts that might bias a survey's score, such as adjustmenting for the differing difficulty of polling certain political races or elements of luck in samples by resampling polls. Transparency Score, on the other hand, measures how openly a pollster shares methodological details like question wording, sampling methods, and weighting procedures. By combining these two metrics into a single star rating, we achieve an excellent overall assessment of a pollster's quality, capturing both their empirical accuracy and their commitment to methodological transparency.
We use this single measurement out of 3 to ensure the integrity of our analysis. Selecting only those polls that originate from reputable pollsters, specifically targeting those with a numeric grade of 2.5 or higher.  This approach is used so that the data reflects a robust and credible representation of voter preferences.

In keeping with this aim, we focused our analysis on polls conducted after September. Early in the election cycle, polls can be significantly influenced by initial campaign events, such as the shocks from the first debates or unexpected changes in the candidate lineup, for example Biden dropping out as a candidate on July 21st. These factors can introduce biases and fluctuations that do not necessarily reflect the enduring preferences of the electorate. By selecting polls from after September, we capture data from a period when voter opinions are more stable and better represent the current state of the race.

```{python}
#| echo: false

# Load the data (replace with your CSV file path)
df = pd.read_csv('../data/02-analysis_data/merged_swing_state_data.csv')

# Group by state and calculate summary statistics
summary_stats = df.groupby('state').agg(
    numeric_grade_mean=('numeric_grade', 'mean'),
    numeric_grade_std=('numeric_grade', 'std'),
    numeric_grade_min=('numeric_grade', 'min'),
    numeric_grade_max=('numeric_grade', 'max'),
    numeric_grade_count=('numeric_grade', 'count'),
    sample_size_mean=('sample_size', 'mean'),
    sample_size_std=('sample_size', 'std'),
    sample_size_min=('sample_size', 'min'),
    sample_size_max=('sample_size', 'max'),
    sample_size_count=('sample_size', 'count')
).reset_index()

# Reshape the summary table to long format
summary_long = pd.melt(
    summary_stats,
    id_vars='state',
    value_vars=[
        'numeric_grade_mean', 'numeric_grade_std', 'numeric_grade_min', 'numeric_grade_max', 'numeric_grade_count',
        'sample_size_mean', 'sample_size_std', 'sample_size_min', 'sample_size_max', 'sample_size_count'
    ],
    var_name='variable_stat', value_name='value'
)

# Split 'variable_stat' into 'variable' and 'stat'
summary_long[['variable', 'stat']] = summary_long['variable_stat'].str.rsplit('_', n=1, expand=True)

# Drop the original column
summary_long.drop('variable_stat', axis=1, inplace=True)

# Pivot the table to get desired output
summary_pivot = summary_long.pivot_table(
    index=['state', 'variable'], columns='stat', values='value', aggfunc='first'
).reset_index()

# Sort for better readability
summary_pivot = summary_pivot.sort_values(by=['state', 'variable'])

summary_pivot[['mean', 'std', 'min', 'max']] = summary_pivot[['mean', 'std', 'min', 'max']].round(1)


# Create a list of rows for tabulate (merging state rows visually)
rows = []
last_state = None
for _, row in summary_pivot.iterrows():
    state = row['state']
    variable = row['variable']
    mean, std, min_, max_, count = row['mean'], row['std'], row['min'], row['max'], row['count']
    
    # Only show state name on the first row of each group
    if state == last_state:
        state = ''
    else:
        last_state = state

    rows.append([state, variable, mean, std, min_, max_, count])

# Print the table using tabulate
print(tabulate(rows, headers=['State', 'Variable', 'Mean', 'Std', 'Min', 'Max', 'Count'], tablefmt='fancy_grid'))
```


 # 2.2 Variables of Interest
In our logistic regression model predicting the 2024 U.S. Presidential Election outcome, the primary variables of interest are derived from aggregated state-level polling data. The percentage of respondant favouring a candidate become the key variables for prediction. Within the collection of polls in our sample the independent variable `trump_pct` represents the percentage of respondents who indicate support for Donald Trump , and `harris_pct` respectively.
Polls ar taking towards the build up of an election and thus ar taken by repondants at different times. Graph  x shows how the sample of polls are distributed over time in which it is clear that we have a collection of polls from a wide range of periods. Because sentiment changes over time we will adjust the weight that we place on polls that are closer to the election as these will be more indicative of how respondants will be voting on the day.
```{python}
#| echo: false

# Load your data (replace with your actual data file path)
data = pd.read_parquet('../data/02-analysis_data/merged_swing_state_data.parquet')

# Ensure date column is in datetime format
data['end_date'] = pd.to_datetime(data['end_date'])

# Plot the histogram of poll quantity by end date
plt.figure(figsize=(12, 6))
plt.hist(data['end_date'], bins=20, edgecolor='black', alpha=0.7)

# Adding labels and title
plt.xlabel('End Date of Poll')
plt.ylabel('Number of Polls')
plt.title('Histogram of Poll Quantity by End Date')
plt.xticks(rotation=45)

# Show the plot
plt.tight_layout()
plt.show()


```

Another aspects of the polls we wish to consider is the population size of each. Larger polls result in a stronger reduction in randomness in the response. The larger the sample size the more the response reflects the true population of the state, this is known as the law of large numbers. In table x we can see the variety within the states polls.
For use in the model the percentages in these polls is weighted for recenecy and size of the population.


 # 3 Model {#sec-model}

 
This paper develops a logistic regression model to predict the likelihood of Donald Trump winning various states in the upcoming 2024 U.S. Presidential Election based on aggregated polling data. The model leverages polling percentages, recency of the polls, and sample sizes to provide a robust probability estimate of Trump’s chances against Kamala Harris.

Model Overview

The model can be expressed mathematically using the logistic function:


$P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2)}}$


Where:

•	 $P(Y=1 | X)$  represents the probability of Trump winning (i.e.,  Y=1 ).
•	 $\beta_0$  is the intercept term.
•	 $\beta_1$  and  $\beta_2$  are the coefficients for the predictor variables, which include:
•	 X_1 : The average polling percentage for Trump ( \text{trump\_pct} ).
•	 X_2 : The average polling percentage for Harris ( \text{harris\_pct} ).

Data and Features

The dataset used in this model is sourced from Project 538, focusing on high-quality polling data. Key features include:

	•	trump_pct: The percentage of respondents supporting Trump.
	•	harris_pct: The percentage of respondents supporting Harris.
	•	sample_size: The size of each poll, which influences the weight of that poll in our analysis.
	•	start_date: The date when the poll began, which is used to calculate recency.

Weight Calculation

To ensure that more recent and larger polls have a higher influence on the model, we calculate a weight for each poll as follows:

Recency Weight:

$\text{recency weight} = \max(\text{current date} - \text{start date}) - (\text{current date} - \text{start date})$

This transformation ensures that more recent polls receive a larger weight.

Total Weight:

$\text{weight} = \text{sample size} \times \text{recency weight}$


The weighted polling percentages for Trump and Harris are then calculated for each state, allowing for a more nuanced aggregation of voter sentiment.

Logistic Regression Implementation

The logistic regression model is implemented using the LogisticRegression class from the sklearn library. The predictor variables are defined as:

	•	 X  = [trump_pct, harris_pct]

The binary outcome variable is defined as follows:

$y = 1$  if  $\text{trump\_pct} > \text{harris\_pct}$  (Trump is predicted to win)
y = 0  otherwise.

Model Validation and Assumptions

The assumptions underlying this model include:

	•	Linearity: The log-odds of the outcome variable are linearly related to the predictor variables.
	•	Independence: Observations are independent of one another.

Limitations

	1.	Polling Bias: The model is contingent upon the quality of the polling data. Polls with lower numeric grades or transparency scores are excluded, potentially introducing bias if high-quality polls are not representative of the entire electorate.
	2.	Dynamic Voter Sentiment: Voter preferences can change rapidly, particularly in the lead-up to the election, and the model may not fully capture these shifts if not updated frequently.

Alternative Models Considered

Alternative models, such as Bayesian logistic regression, were considered. While Bayesian models allow for the incorporation of prior distributions and might provide additional insights through credible intervals, they require more complex implementation and careful selection of priors. Given the nature of the data and the objective of clear interpretability, the logistic regression model was chosen for its straightforward application and interpretability.

Conclusion

The logistic regression model provides a structured approach to predict the likelihood of Trump winning in various states, based on recent and high-quality polling data. By focusing on key features such as polling percentages and sample sizes, the model captures the competitive dynamics between candidates and offers valuable insights into potential election outcomes. Future iterations of this model can incorporate real-time data updates to enhance predictive accuracy as the election approaches.

 # 4 Results {#sec-results}
The results of our analysis are as follows. We have Donald Trump predicted to win Arizona, Georgia and Michigan for a total of 42 of the 93 swing state votes. While Kamala Harris is projected to win Nevada, North Carolina, Pennsylvania and Wisconsin for the remaining 51 swing state votes. This results in a win for the democrats and the first female US president!


# 5 Discussion {#sec-discussion}

## What Is Done in This Paper?

In this paper, we create a logistic regression model to predict the outcome of the 2024 U.S. Presidential Election, focusing on state-level polling data to estimate the probability of Donald Trump winning against Kamala Harris. By aggregating recent and reliable polls for swing states and weighting them by sample size and recency, we aim to provide a data-driven analysis of voter preferences. Our model is constructed to identify the likelihood of Trump securing a majority in each state, ultimately offering a prediction for the overall election result.

## What Do We Learn About the World?

One of the most significant insights from this analysis is the discrepancy between polling data and the betting markets. While the majority of reputable polls suggest that Kamala Harris is favored on both a national level and in most swing states, betting markets like Polymarket imply a 67% chance of Trump winning the election as of October 23, 2024 [@polymarket]. This stark disconnect raises questions about the accuracy of traditional polling methods and whether betting markets, which are financial tools driven by market forces, may offer a more precise reflection of public sentiment.

At first glance, one might expect that if polling data were more accurate than betting markets, arbitrage opportunities would emerge, allowing savvy participants to profit from discrepancies. However, this does not seem to be happening, which suggests that the markets may be pricing in information that the polls do not capture—perhaps reflecting shifts in voter sentiment, hidden preferences, or systematic biases in polling.

## What Else Do We Learn About the World?

Another important consideration is the possible biases in both polling and betting markets. Poll respondents may not be a representative sample of the electorate; for example, Democrats could be more likely to respond to polls, skewing the results in favor of Harris. Meanwhile, individuals who participate in betting markets might form a subset of the population that is disproportionately supportive of Trump, which could explain why the implied odds heavily favor him. Furthermore, the rapid expansion and increased liquidity of betting markets in the last few years may have improved their efficiency, making them more reflective of real-world probabilities. However, historical data still suggest that polls have been accurate 78% of the time in predicting elections [@FiveThirtyEight], indicating that the reliability of betting markets remains questionable, especially in light of their poor performance during the 2016 election, when Trump’s odds were listed at +475 (17%) just before his victory [https://www.oddsshark.com/entertainment/us-presidential-odds-2016-futures].

## Weaknesses of the Model

While our model provides a structured framework for predicting the election, it has several limitations. First, the model is based on polling data available far in advance of the election, meaning there is a high degree of uncertainty, and the model may not fully capture late shifts in public opinion or external shocks (e.g., economic downturns or scandals). Additionally, the use of linear modeling may oversimplify the complex dynamics of voter behavior, as elections often involve non-linear influences that are difficult to predict.

Moreover, our focus on swing states limits the model’s applicability to the national picture. While swing states are crucial to the election outcome, non-swing states could offer additional insights into broader voter trends, and including them in future analyses could enhance the model’s robustness. Training the model on a more comprehensive dataset that includes these states and other predictive variables, such as demographic factors or economic indicators, could lead to a more accurate forecast.

## How Should We Proceed in the Future?

Looking ahead, future iterations of this model should incorporate more diverse data sources. Expanding the dataset to include polling data from all states, along with betting market information, could improve prediction accuracy. Moreover, experimenting with different types of predictive models, such as Bayesian logistic regression or machine learning models like random forests, could offer more sophisticated insights and account for non-linear interactions that our current model might miss.

It would also be valuable to study the interaction between polling data and betting markets more closely, potentially integrating them into a unified prediction model. This hybrid approach might help reconcile the discrepancies observed between the two sources and provide a more nuanced understanding of election dynamics. Additionally, out-of-sample validation and sensitivity analyses should be conducted to test the robustness of our model and adjust for overfitting.

Ultimately, while our model offers a strong foundation for predicting the 2024 U.S. Presidential Election, there is still much to learn. By refining the model and incorporating additional data, we can move closer to producing predictions that more accurately reflect voter behavior and election outcomes.

# Appendix A: Idealized Methodology and Survey for Forecasting the US Presidential Election with Incentivized Betting

## 1. Sampling Approach

To ensure that our sample represents the diversity of the US electorate and minimizes potential biases, we employ stratified and cluster sampling techniques. With a budget of $100,000, our goal is to gather data from 5,000 respondents, focusing on key swing states while preserving national representativeness. The sample will be stratified by:

- **Demographics**: Race, age, gender, education, and income.
- **Geography**: Emphasis on swing states with representation from urban, suburban, and rural areas.
- **Voter History**: Including respondents with varying voting patterns, such as frequent voters, occasional voters, and those who are less likely to vote.

To address geographic and political biases, cluster sampling will target diverse regions within each state (urban, suburban, and rural areas), capturing the variation in political leanings. Additionally, we will soft-launch the survey in order to catch potential issues. 

## 2. Recruitment of Respondents

Respondents will be recruited through a multi-channel outreach strategy that combines online and telephone efforts to ensure a representative sample:

- **Online Recruitment**: Targeted ads on social media, political forums, and news websites will attract a wide audience, highlighting the unique opportunity to place a small bet on the election outcome.
- **Telephone Recruitment**: To capture older demographics and those less reachable online, we will conduct phone outreach using commercial databases and voter registration information.

### Incentives for Engagement
A betting pool is a central part of this methodology, with $50,000 allocated as rewards for respondents who accurately predict the election outcome. Each participant will receive a $10 allowance to bet on their predicted winner (Trump or Harris), with payouts based on live odds, creating a direct incentive for respondents to make thoughtful, informed predictions. The purpose of this is twofold, it aims to minmize the attrition of the respondants, so that more people complete the survey, and it also will provide us insight into who people think will win rather than just who people want to win. Ideally, the purpose of this is so that people take into consideration who their friends and family, as well as their community are expecting to win. 

## 3. Data Validation and Bias Reduction

To further reduce biases, several measures will be implemented for data validation and weighting:

- **Cross-Referencing Survey Responses**: Survey responses will be cross-referenced with voter registration data to validate eligibility.
- **Weighting**: Data will be weighted to reflect broader population demographics, adjusting for any imbalances in representation across age, race, gender, and geographic factors. This ensures that the results better mirror the entire electorate.

## 4. Poll Aggregation

Data collected through this survey will be aggregated with other national and state-level surveys to improve accuracy. The aggregation process will account for:

- **Sample Size and Recency**: More recent polls and those with larger sample sizes will be weighted more heavily.
- **Pollster Reliability**: Historical poll accuracy and transparency scores will further inform the weight of each poll.

## 5. Budget Allocation

The proposed $100,000 budget will be distributed as follows:

- **Recruitment**: $10,000 for targeting and engaging respondents across multiple platforms.
- **Survey Administration (Online and Phone)**: $20,000 for both online and phone-based survey collection.
- **Betting Pool**: $50,000 allocated to reward accurate predictions and enhance response quality.
- **Data Validation and Analysis**: $10,000 for verification and weighting.
- **Modeling**: $10,000 for analyzing and forecasting based on aggregated data.

## 6. Question Design and Goal Allignment
The goal of our survey is to predict who will win the election, this is the research question that guides our survey. In order to achieve this, our questions will be isolate the relevant variables by asking only one thing at a time (ceteris paribus). We will also use item specific scales over agree-disagree formats to avoid acquiescence bias. Furthermore, we will randomize response options in order to mitigate response order bias, and emphasize the surveys anonymity to minimize social desireablity bias. 

This approach, inspired by [@Stantcheva’s guide on survey creation](https://docs.google.com/forms/d/e/1FAIpQLSc5LYCAd0OmiIc-LHRFUVgVGuSMKzayVWZB7VOrsrDOe5742w/viewform?vc=0&c=0&w=1&flr=0), leverages financial incentives to align respondent predictions with their genuine expectations. By combining innovative sampling, incentivized engagement, and rigorous data validation, this methodology aims to bridge the gap between traditional polling and betting market predictions, ultimately enhancing the accuracy of our election forecast.

# appndix B 
say what kind of ampling. 
reference rsearch on wha the sampling mthod is 


# References


